{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split sentense into words 字\n",
    "import sys\n",
    "import numpy as np\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/liuqiao/PR_project/data'\n",
    "vocab = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines_train = open('%s/train.txt'%DATA_PATH).readlines()\n",
    "first_line = lines_train[0]\n",
    "for i in range(len(lines_train)):\n",
    "    line = lines_train[i]\n",
    "    if line.decode('UTF-8').rstrip('\\n').rstrip('\\r').isdigit() or i==0:\n",
    "        pass\n",
    "    else:\n",
    "        for each in line.decode('UTF-8').rstrip('\\n').rstrip('\\r'):\n",
    "            #print each.encode('UTF-8')\n",
    "            vocab.append(each.encode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines_valid = open('%s/valid.txt'%DATA_PATH).readlines()\n",
    "for i in range(len(lines_valid)):\n",
    "    line = lines_valid[i]\n",
    "    if line.decode('UTF-8').rstrip('\\n').rstrip('\\r').isdigit() or i==0:\n",
    "        pass\n",
    "    else:\n",
    "        for each in line.decode('UTF-8').rstrip('\\n').rstrip('\\r'):\n",
    "            #print each.encode('UTF-8')\n",
    "            vocab.append(each.encode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_uniq = set(vocab)\n",
    "list_freq = []\n",
    "for each in vocab_uniq:\n",
    "    list_freq.append([each,vocab.count(each)])\n",
    "list_freq = sorted(list_freq, key=lambda k: k[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_freq_filter = list_freq[:-399]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_out = open('%s/vocabulary_runtime2.txt'%DATA_PATH,'w')\n",
    "for each in list_freq_filter:\n",
    "    f_out.write('%s\\n'%each[0])\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic_voc = {}\n",
    "for i in range(len(list_freq_filter)):\n",
    "    dic_voc[list_freq_filter[i][0].decode('UTF-8')] = float(i+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#coco caption_generate\n",
    "import json\n",
    "DATA_PATH = '/home/liuqiao/PR_project/data'\n",
    "lines_train = open('%s/train.txt'%DATA_PATH).readlines()\n",
    "anno_train = {}\n",
    "info={}\n",
    "annotations={}\n",
    "info[u'contributor'] = u'WWT&LQ'\n",
    "info[u'date_created'] = u'2017-04-25'\n",
    "info[u'description'] = u'This is the 2017 Chinese image caption training dataset from PR class'\n",
    "anno_train[u'info'] = info\n",
    "anno_train[u'images']=[]\n",
    "anno_train[u'annotations']=[]\n",
    "image_id = 1\n",
    "anno_id = 1\n",
    "for i in range(len(lines_train)):\n",
    "    line = lines_train[i]\n",
    "    if line.decode('UTF-8').rstrip('\\n').rstrip('\\r').isdigit() or i==0:\n",
    "        images={}\n",
    "        if i==0:\n",
    "            image_id = 1\n",
    "        else:\n",
    "            image_id = int(line.decode('UTF-8').rstrip('\\n').rstrip('\\r'))\n",
    "        images[u'id'] = image_id\n",
    "        anno_train[u'images'].append(images)\n",
    "    else:\n",
    "        anno={}\n",
    "        sentence = []\n",
    "        for each in line.decode('UTF-8').rstrip('\\n').rstrip('\\r'):\n",
    "            try:\n",
    "                sentence.append(str(dic_voc[each]))\n",
    "            except KeyError:\n",
    "                sentence.append(str(1.0))\n",
    "        cap_encode = ' '.join(sentence)\n",
    "        anno[u'image_id'] = image_id\n",
    "        anno[u'id'] = anno_id\n",
    "        anno[u'caption'] = u'%s'%cap_encode\n",
    "        anno_train[u'annotations'].append(anno)\n",
    "        anno_id += 1\n",
    "\n",
    "f_anno_train = open('%s/captions_train2.json'%DATA_PATH,'w')\n",
    "json.dump(anno_train,f_anno_train)\n",
    "f_anno_train.close()\n",
    "        \n",
    "lines_valid = open('%s/valid.txt'%DATA_PATH).readlines()\n",
    "anno_valid = {}\n",
    "info={}\n",
    "annotations={}\n",
    "info[u'contributor'] = u'WWT&LQ'\n",
    "info[u'date_created'] = u'2017-04-25'\n",
    "info[u'description'] = u'This is the 2017 Chinese image caption validation dataset from PR class'\n",
    "anno_valid[u'info'] = info\n",
    "anno_valid[u'images']=[]\n",
    "anno_valid[u'annotations']=[]\n",
    "image_id = 8001\n",
    "\n",
    "for i in range(len(lines_valid)):\n",
    "    line = lines_valid[i]\n",
    "    if line.decode('UTF-8').rstrip('\\n').rstrip('\\r').isdigit() or i==0:\n",
    "        images={}\n",
    "        if i==0:\n",
    "            image_id = 8001\n",
    "        else:\n",
    "            image_id = int(line.decode('UTF-8').rstrip('\\n').rstrip('\\r'))\n",
    "        images[u'id'] = image_id\n",
    "        anno_valid[u'images'].append(images)\n",
    "    else:\n",
    "        anno={}\n",
    "        sentence = []\n",
    "        for each in line.decode('UTF-8').rstrip('\\n').rstrip('\\r'):\n",
    "            try:\n",
    "                sentence.append(str(dic_voc[each]))\n",
    "            except KeyError:\n",
    "                sentence.append(str(1.0))\n",
    "        cap_encode = ' '.join(sentence)\n",
    "        anno[u'image_id'] = image_id\n",
    "        anno[u'id'] = anno_id\n",
    "        anno[u'caption'] = u'%s'%cap_encode\n",
    "        anno_valid[u'annotations'].append(anno)\n",
    "        anno_id += 1\n",
    "        \n",
    "\n",
    "f_anno_valid = open('%s/captions_valid2.json'%DATA_PATH,'w')\n",
    "json.dump(anno_valid,f_anno_valid)\n",
    "f_anno_valid.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.16s)\n",
      "creating index...\n",
      "index created!\n",
      "37.0 135.0 3.0 23.0 21.0 6.0 11.0 2.0 5.0 21.0 130.0 192.0 227.0 103.0\n",
      "2.0 275.0 60.0 9.0 19.0 6.0 11.0 55.0 46.0 21.0 1.0\n",
      "200.0 5.0 221.0 227.0 3.0 192.0 148.0 103.0\n",
      "24.0 10.0 216.0 222.0 59.0 675.0 5.0 21.0 130.0 103.0\n",
      "98.0 21.0 6.0 11.0 55.0 46.0 70.0 10.0 3.0 130.0 249.0 103.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annFile = '/home/liuqiao/PR_project/data/captions_train2.json'\n",
    "caps=COCO(annFile)\n",
    "annIds = caps.getAnnIds(imgIds=1);\n",
    "anns = caps.loadAnns(annIds)\n",
    "caps.showAnns(anns)\n",
    "annIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.0 135.0 3.0 23.0 21.0 6.0 11.0 2.0 5.0 21.0 130.0 192.0 227.0 103.0\n"
     ]
    }
   ],
   "source": [
    "sentence=[]\n",
    "for each in '大街的马路上有一个路标指向牌'.decode('UTF-8').rstrip('\\n').rstrip('\\r'):\n",
    "    sentence.append(str(dic_voc[each]))\n",
    "cap_encode = ' '.join(sentence)\n",
    "print cap_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_voc['街'.decode('UTF-8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38445 [['5400', '25918'], ['1946', '9330'], ['5600', '26876']]\n"
     ]
    }
   ],
   "source": [
    "#generate train h5 file\n",
    "image_cap_list = []\n",
    "for line in open('/home/liuqiao/PR_project/data/image2cap_train.txt').readlines():\n",
    "    img_id, cap_id = line.rstrip('\\n').split('\\t')\n",
    "    image_cap_list.append([img_id,cap_id])\n",
    "print len(image_cap_list),image_cap_list[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "f_train = h5py.File(\"/home/liuqiao/PR_project/data/captions_train2.h5\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "b_size = f_train.create_dataset(u'buffer_size', (1,), dtype='i8')\n",
    "b_size[...] = 100\n",
    "input_sentence = -np.ones((7700,100)) #(38445/100+1)*20\n",
    "for i in range(385):\n",
    "    input_sentence[20*i] = np.zeros(100)\n",
    "image_cap_list = []\n",
    "for line in open('/home/liuqiao/PR_project/data/image2cap_train.txt').readlines():\n",
    "    img_id, cap_id = line.rstrip('\\n').split('\\t')\n",
    "    image_cap_list.append([img_id,cap_id])\n",
    "for i in range(len(image_cap_list)):\n",
    "    img_id, cap_id = image_cap_list[i]\n",
    "    annIds = caps.getAnnIds(imgIds=int(img_id))\n",
    "    #if cap_id==25918:\n",
    "     #   print img_id\n",
    "    caption = caps.loadAnns(annIds[annIds.index(int(cap_id))])[0]['caption']\n",
    "    cap_list = caption.split(' ')\n",
    "    #print cap_list\n",
    "    #sys.exit()\n",
    "    \n",
    "    batch_th = i/100\n",
    "    start_row = 20*batch_th\n",
    "    start_col = i-batch_th*100\n",
    "    \n",
    "    for j in range(np.min([len(cap_list),19])):\n",
    "        try:\n",
    "            input_sentence[start_row+j+1][start_col] = float(cap_list[j])#list out of range\n",
    "        except IndexError:\n",
    "            print start_row,start_col,i,j,len(cap_list)\n",
    "            sys.exit()\n",
    "input_seq = f_train.create_dataset(u'input_sentence', (7700,100), dtype='f8')\n",
    "input_seq[...] = input_sentence\n",
    "\n",
    "target_sentence = -np.ones((7700,100))\n",
    "for i in range(385):\n",
    "    for j in range(100):\n",
    "        for k in range(19):\n",
    "            if input_sentence[20*i+k+1][j]!=-1:\n",
    "                target_sentence[20*i+k][j] = input_sentence[20*i+k+1][j]\n",
    "            else:\n",
    "                target_sentence[20*i+k][j] = 0\n",
    "                break  \n",
    "target_seq = f_train.create_dataset(u'target_sentence', (7700,100), dtype='f8')\n",
    "target_seq[...] = target_sentence\n",
    "cont_sentence = np.zeros([7700,100])\n",
    "for i in range(7700):\n",
    "    for j in range(100):\n",
    "        if input_sentence[i][j] != -1 and input_sentence[i][j]!=0:\n",
    "            cont_sentence[i][j]=1\n",
    "con_seq = f_train.create_dataset(u'cont_sentence', (7700,100), dtype='f8')\n",
    "con_seq[...] = cont_sentence\n",
    "f_train.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "2.0 25.0 69.0 10.0 3.0 37.0 138.0 168.0 224.0 65.0 520.0 796.0\n",
      "2.0 25.0 69.0 10.0 3.0 138.0 101.0 92.0 121.0 210.0\n",
      "2.0 25.0 37.0 138.0 12.0 263.0 988.0 1.0 520.0 168.0 224.0 210.0\n",
      "2.0 25.0 69.0 10.0 37.0 138.0 35.0 92.0 121.0 2.0 5.0 24.0 67.0 194.0 45.0 3.0 103.0 1973.0\n",
      "24.0 10.0 3.0 216.0 222.0 71.0 287.0 607.0 11.0 2.0 5.0 33.0 10.0 3.0 481.0 357.0 74.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[38446, 38447, 38448, 38449, 38450]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annFile = '/home/liuqiao/PR_project/data/captions_valid2.json'\n",
    "caps=COCO(annFile)\n",
    "annIds = caps.getAnnIds(imgIds=8001);\n",
    "anns = caps.loadAnns(annIds)\n",
    "caps.showAnns(anns)\n",
    "annIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate test h5 file\n",
    "f_valid = h5py.File(\"/home/liuqiao/PR_project/data/captions_valid2.h5\", \"w\")\n",
    "b_s = f_valid.create_dataset(u'buffer_size', (1,), dtype='i8')\n",
    "b_s[...] = 100\n",
    "input_sentence = -np.ones((980,100)) #(4811/100+1)*20\n",
    "for i in range(49):\n",
    "    input_sentence[20*i] = np.zeros(100)\n",
    "image_cap_list = []\n",
    "for line in open('/home/liuqiao/PR_project/data/image2cap_valid.txt').readlines():\n",
    "    img_id, cap_id = line.rstrip('\\n').split('\\t')\n",
    "    image_cap_list.append([img_id,cap_id])\n",
    "for i in range(len(image_cap_list)):\n",
    "    img_id, cap_id = image_cap_list[i]\n",
    "    annIds = caps.getAnnIds(imgIds=int(img_id))\n",
    "    #if cap_id==25918:\n",
    "     #   print img_id\n",
    "    caption = caps.loadAnns(annIds[annIds.index(int(cap_id))])[0]['caption']\n",
    "    cap_list = caption.split(' ')\n",
    "    batch_th = i/100\n",
    "    start_row = 20*batch_th\n",
    "    start_col = i-batch_th*100\n",
    "    for j in range(np.min([len(cap_list),19])):\n",
    "        try:\n",
    "            input_sentence[start_row+j+1][start_col] = float(cap_list[j])#list out of range\n",
    "        except IndexError:\n",
    "            print start_row,start_col,i,j,len(cap_list)\n",
    "            sys.exit()\n",
    "input_seq = f_valid.create_dataset(u'input_sentence', (980,100), dtype='f8')\n",
    "input_seq[...] = input_sentence\n",
    "\n",
    "target_sentence = -np.ones((980,100)) #(4811/100+1)*20\n",
    "for i in range(49):\n",
    "    for j in range(100):\n",
    "        for k in range(19):\n",
    "            if input_sentence[20*i+k+1][j]!=-1:\n",
    "                target_sentence[20*i+k][j] = input_sentence[20*i+k+1][j]\n",
    "            else:\n",
    "                target_sentence[20*i+k][j] = 0\n",
    "                break \n",
    "target_seq = f_valid.create_dataset(u'target_sentence', (980,100), dtype='f8')\n",
    "target_seq[...] = target_sentence\n",
    "cont_sentence = np.zeros([980,100])\n",
    "for i in range(980):\n",
    "    for j in range(100):\n",
    "        if input_sentence[i][j] != -1 and input_sentence[i][j] != 0:\n",
    "            cont_sentence[i][j]=1\n",
    "con_seq = f_valid.create_dataset(u'cont_sentence', (980,100), dtype='f8')\n",
    "con_seq[...] = cont_sentence\n",
    "f_valid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "三个人在玩棒球其中两人身着红色棒球帽，红色上衣，白底红条纹裤子另外一个人着绿帽子，绿上衣和浅灰色裤子\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "L = np.zeros(len(lines_train))\n",
    "index = 0\n",
    "for line in lines_train:\n",
    "    L[index] = len(line.decode('UTF-8').rstrip('\\n').rstrip('\\r'))\n",
    "    if L[index]==50:\n",
    "        print line\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_h51=h5py.File('/home/liuqiao/PR_project/data/captions_train2.h5','r')\n",
    "f_h52=h5py.File('/home/liuqiao/PR_project/data/captions_valid2.h5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_train_val = h5py.File(\"/home/liuqiao/PR_project/data/captions_train_val2.h5\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8680, 100)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_s = f_train_val.create_dataset(u'buffer_size', (1,), dtype='i8')\n",
    "b_s[...] = 100\n",
    "input_seq1 = f_h51['input_sentence']\n",
    "input_seq2 = f_h52['input_sentence']\n",
    "input_seq3 = np.concatenate((input_seq1, input_seq2), axis=0)\n",
    "input_seq3.shape\n",
    "input_seq = f_train_val.create_dataset(u'input_sentence', (8680,100), dtype='f8')\n",
    "input_seq[...]=input_seq3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8680, 100)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_target1 = f_h51['target_sentence']\n",
    "input_target2 = f_h52['target_sentence']\n",
    "input_target3 = np.concatenate((input_target1, input_target2), axis=0)\n",
    "input_target3.shape\n",
    "target_seq = f_train_val.create_dataset(u'target_sentence', (8680,100), dtype='f8')\n",
    "target_seq[...]=input_target3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8680, 100)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_cont1 = f_h51['cont_sentence']\n",
    "input_cont2 = f_h52['cont_sentence']\n",
    "input_cont3 = np.concatenate((input_cont1, input_cont2), axis=0)\n",
    "input_cont3.shape\n",
    "cont_seq = f_train_val.create_dataset(u'cont_sentence', (8680,100), dtype='f8')\n",
    "cont_seq[...]=input_cont3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'buffer_size', u'cont_sentence', u'input_sentence', u'target_sentence']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_train_val.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_train_val.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
